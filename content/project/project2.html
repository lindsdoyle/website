---
title: 'Project 2: Modeling, Testing, and Predicting'
author: "SDS348"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
---



<div id="modeling" class="section level1">
<h1>Modeling</h1>
<p><strong>Lindsey Doyle LND642</strong>
- <strong>0. Introduction</strong></p>
<p><strong>The dataset that is used for this project aims to look at how different explanatory variables influence the distance of the college a student chooses to attend from their hometown.The survey is collected on students in 1980 from approximately 1,100 high schools and was conducted by the U.S. Department of Education.There are 4739 observations of 15 variables in the dataset. The variables is 4-year college distance from home,gender, ethnicity, composite test score, level of education of both parents (mcollege and fcollege), whether or not the family owns a home (home) and where the home is located (urban), the unemployment rate and state hourly wage for the county, the average state 4 year tuition, the individual’s number of years in education, family income, and region.</strong>
<strong>Citation: Kleiber, Christian. “College Distance Data.” Rdrr.io, 6 Feb. 2020, rdrr.io/cran/AER/man/CollegeDistance.html. Accessed 28 Apr. 2021.</strong></p>
<pre class="r"><code>CollegeDistance &lt;- read.csv(&quot;~/CollegeDistance (1).csv&quot;) </code></pre>
<ul>
<li><strong>1. MANOVA &amp; ANOVA</strong></li>
</ul>
<pre class="r"><code>group&lt;-CollegeDistance$income
DV &lt;- CollegeDistance %&gt;% select(score, education, unemp)
sapply(split(DV,group), mshapiro_test)</code></pre>
<pre><code>##           high         low        
## statistic 0.9292558    0.9273997  
## p.value   6.966493e-25 1.19527e-37</code></pre>
<pre class="r"><code>box_m(DV,group)</code></pre>
<pre><code>## # A tibble: 1 x 4
## statistic p.value parameter method
## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;
## 1 16.1 0.0131 6 Box&#39;s M-test for Homogeneity of
Covariance Matrices</code></pre>
<pre class="r"><code>man1&lt;-manova(cbind(score, education, unemp)~income, data=CollegeDistance)
summary(man1)</code></pre>
<pre><code>## Df Pillai approx F num Df den Df Pr(&gt;F)
## income 1 0.060832 102.23 3 4735 &lt; 2.2e-16 ***
## Residuals 4737
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1</code></pre>
<pre class="r"><code>summary.aov(man1)</code></pre>
<pre><code>## Response score :
## Df Sum Sq Mean Sq F value Pr(&gt;F)
## income 1 11415 11414.6 155.66 &lt; 2.2e-16 ***
## Residuals 4737 347362 73.3
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## Response education :
## Df Sum Sq Mean Sq F value Pr(&gt;F)
## income 1 728.5 728.47 239.02 &lt; 2.2e-16 ***
## Residuals 4737 14437.4 3.05
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## Response unemp :
## Df Sum Sq Mean Sq F value Pr(&gt;F)
## income 1 221 220.866 29.091 7.243e-08 ***
## Residuals 4737 35965 7.592
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1</code></pre>
<pre class="r"><code>CollegeDistance%&gt;%group_by(income)%&gt;%summarize(mean(score),mean(education), mean(unemp))</code></pre>
<pre><code>## # A tibble: 2 x 4
##   income `mean(score)` `mean(education)` `mean(unemp)`
##   &lt;fct&gt;          &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;
## 1 high            53.3              14.4          7.26
## 2 low             49.9              13.6          7.73</code></pre>
<pre class="r"><code>pairwise.t.test(CollegeDistance$score,CollegeDistance$education,CollegeDistance$unemp, p.adj=&quot;none&quot;)</code></pre>
<pre><code>##
## Pairwise comparisons using t tests with pooled SD
##
## data: CollegeDistance$score and
CollegeDistance$education
##
## 12 13 14 15 16 17
## 13 1.7e-15 - - - - -
## 14 &lt; 2e-16 0.00100 - - - -
## 15 &lt; 2e-16 &lt; 2e-16 1.0e-08 - - -
## 16 &lt; 2e-16 &lt; 2e-16 &lt; 2e-16 9.1e-10 - -
## 17 &lt; 2e-16 &lt; 2e-16 &lt; 2e-16 7.9e-15 0.00027 -
## 18 &lt; 2e-16 8.4e-08 9.1e-05 0.15844 0.32504 0.00739
##
## P value adjustment method: none</code></pre>
<pre class="r"><code>1-.95^6</code></pre>
<pre><code>## [1] 0.2649081</code></pre>
<pre class="r"><code>.05/6</code></pre>
<pre><code>## [1] 0.008333333</code></pre>
<p><strong>A MANOVA, an ANOVA, and 4 post-hoc tests were can for a total of 6. The probability of at least one type I error is 26.49% and the bonferroni correction is 0.008, meaning that it is still significant. All MANOVA assumptions were met because each group has over 25 subjects. There is a significant difference between low income and high income households for unemployement rate, assessment score, and education level.</strong></p>
<ul>
<li><strong>2. Randomization Test</strong></li>
</ul>
<pre class="r"><code>set.seed(1234)
summary(aov(education~gender, data=CollegeDistance))</code></pre>
<pre><code>##               Df Sum Sq Mean Sq F value Pr(&gt;F)
## gender         1      1   1.446   0.452  0.502
## Residuals   4737  15164   3.201</code></pre>
<pre class="r"><code>OBS_F &lt;-0.452
Fs &lt;- replicate(5000,{
  new &lt;- CollegeDistance%&gt;%mutate(education=sample(education))
  SSW &lt;- new%&gt;%group_by(gender)%&gt;%summarize(SSW=sum((education-mean(education))^2))%&gt;%summarize(sum(SSW))%&gt;%pull
  SSB &lt;- new%&gt;%mutate(mean=mean(education))%&gt;%group_by(gender)%&gt;%mutate(groupmean=mean(education))%&gt;%summarize(SSB=sum((mean-groupmean)^2))%&gt;%summarize(sum(SSB))%&gt;%pull
  (SSB/1)/(SSW/4737)
})

hist(Fs, probs=T); abline(v= OBS_F, col=&quot;orange&quot;, add=T)</code></pre>
<p><img src="/project/project2_files/figure-html/unnamed-chunk-3-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>mean(Fs&gt;OBS_F)</code></pre>
<pre><code>## [1] 0.5004</code></pre>
<p><strong>The null hypothesis is that there is no difference between education levels between genders. The alternative hypothesis is that there is a difference between education levels between genders. The p-value is 0.5004, which means I accept the null hypothesis.</strong></p>
<ul>
<li><strong>3. Linear Regression Model</strong> Build a linear regression model predicting one of your response variables from at least 2 other variables, including their interaction. Mean-center any numeric variables involved in the interaction.</li>
</ul>
<pre class="r"><code>CollegeDistance$distance_c &lt;- CollegeDistance$distance - mean(CollegeDistance$distance)
CollegeDistance$score_c &lt;- CollegeDistance$score - mean(CollegeDistance$score)
fit &lt;- lm(distance_c~score_c*urban, data=CollegeDistance)
summary(fit)</code></pre>
<pre><code>##
## Call:
## lm(formula = distance_c ~ score_c * urban, data =
CollegeDistance)
##
## Residuals:
## Min 1Q Median 3Q Max
## -2.7557 -1.3939 -0.3957 0.4136 18.1342
##
## Coefficients:
## Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 0.379565 0.036289 10.459 &lt; 2e-16 ***
## score_c -0.033119 0.004212 -7.863 4.62e-15 ***
## urbanyes -1.582541 0.075778 -20.884 &lt; 2e-16 ***
## score_c:urbanyes 0.034792 0.008524 4.082 4.54e-05 ***
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## Residual standard error: 2.185 on 4735 degrees of
freedom
## Multiple R-squared: 0.09544, Adjusted R-squared: 0.09487
## F-statistic: 166.5 on 3 and 4735 DF, p-value: &lt; 2.2e-16</code></pre>
<p><strong>The linear regression was run after centering the assessment scores and distance from 4 year college.The odds of the distance of a 4-year college from a student’s home decreases by 0.033 per point on the assessment, decreases by 1.583 if the student is from an urban area, and increases by 0.034 if the student is from an urban area and took the assessment test.</strong></p>
<pre class="r"><code>ggplot(CollegeDistance, aes(x=distance_c, y=score_c))+geom_point(aes(color=urban))+  geom_smooth(method=&quot;lm&quot;,se=F,fullrange=T,aes(color=urban))+theme(legend.position=c(.9,.19))</code></pre>
<p><img src="/project/project2_files/figure-html/unnamed-chunk-5-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>resids&lt;-fit$residuals
fitvals&lt;-fit$fitted.values
ggplot()+geom_point(aes(fitvals,resids))+geom_hline(yintercept=0, color=&#39;red&#39;)</code></pre>
<p><img src="/project/project2_files/figure-html/unnamed-chunk-6-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ks.test(resids, &quot;pnorm&quot;, mean=0, sd(resids)) </code></pre>
<pre><code>## 
##  One-sample Kolmogorov-Smirnov test
## 
## data:  resids
## D = 0.17871, p-value &lt; 2.2e-16
## alternative hypothesis: two-sided</code></pre>
<pre class="r"><code>(sum((CollegeDistance$distance_c-mean(CollegeDistance$distance_c))^2)-sum(fit$residuals^2))/sum((CollegeDistance$distance_c)^2)</code></pre>
<pre><code>## [1] 0.09544241</code></pre>
<pre class="r"><code>bptest(fit)</code></pre>
<pre><code>## 
##  studentized Breusch-Pagan test
## 
## data:  fit
## BP = 85.648, df = 3, p-value &lt; 2.2e-16</code></pre>
<pre class="r"><code>coeftest(fit, vcov = vcovHC(fit))[,1:2]</code></pre>
<pre><code>##                     Estimate  Std. Error
## (Intercept)       0.37956467 0.041373352
## score_c          -0.03311929 0.004601847
## urbanyes         -1.58254141 0.045200956
## score_c:urbanyes  0.03479174 0.005117715</code></pre>
<pre class="r"><code>summary(fit)$coef[,1:2]</code></pre>
<pre><code>##                     Estimate  Std. Error
## (Intercept)       0.37956467 0.036289148
## score_c          -0.03311929 0.004212184
## urbanyes         -1.58254141 0.075778492
## score_c:urbanyes  0.03479174 0.008523686</code></pre>
<p><strong>The model explains 9.54% of the variation in the outcome. The model also does not pass the all of the assumptions. The model does not have a normal distribution, it is not homoskedastic, and the relationship is not linear. There is also no significant standard errors for either the original or robust versions. The original and robust errors are relatively unchanged from one another </strong></p>
<ul>
<li><strong>4.Bootstrapped</strong></li>
</ul>
<pre class="r"><code>set.seed(1234)
samp_distn&lt;-replicate(5000, {
boot &lt;- sample_frac(CollegeDistance, replace=TRUE) 
fit &lt;-lm(distance_c~score_c*urban, data= boot)  
coef(fit) #save coefs
})

samp_distn %&gt;% t %&gt;% as.data.frame %&gt;% summarize_all(sd)</code></pre>
<pre><code>##   (Intercept)     score_c   urbanyes score_c:urbanyes
## 1  0.04211341 0.004580159 0.04566567       0.00505419</code></pre>
<pre class="r"><code>coeftest(fit, vcov = vcovHC(fit))[,1:2]</code></pre>
<pre><code>##                     Estimate  Std. Error
## (Intercept)       0.37956467 0.041373352
## score_c          -0.03311929 0.004601847
## urbanyes         -1.58254141 0.045200956
## score_c:urbanyes  0.03479174 0.005117715</code></pre>
<pre class="r"><code>summary(fit)$coef[,1:2]</code></pre>
<pre><code>##                     Estimate  Std. Error
## (Intercept)       0.37956467 0.036289148
## score_c          -0.03311929 0.004212184
## urbanyes         -1.58254141 0.075778492
## score_c:urbanyes  0.03479174 0.008523686</code></pre>
<pre class="r"><code>samp_distn %&gt;% t %&gt;% as.data.frame %&gt;% pivot_longer(1:3) %&gt;% group_by(name) %&gt;% summarize(lower=quantile(value,.025), upper=quantile(value,.975))</code></pre>
<pre><code>## # A tibble: 3 x 3
##   name          lower   upper
##   &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept)  0.299   0.462 
## 2 score_c     -0.0421 -0.0241
## 3 urbanyes    -1.67   -1.49</code></pre>
<p><strong>The bootstrapped standard errors did not significantly change compared to the originial. None were significant. </strong>
- <strong>5. Logistic Regression Model</strong></p>
<pre class="r"><code>#Predicting income from distance of college from home and score
fit &lt;- glm(income~distance+score, data=CollegeDistance, family = &quot;binomial&quot;)
prob &lt;- predict(fit, type = &quot;response&quot;)
class_diag(prob,CollegeDistance$income)</code></pre>
<pre><code>##           acc      sens        spec       ppv       auc
## low 0.7132306 0.9988145 0.007326007 0.7132275 0.6220797</code></pre>
<pre class="r"><code>truth &lt;- CollegeDistance$income
table(prediction = as.numeric(prob &gt; 0.5), truth)</code></pre>
<pre><code>##           truth
## prediction high  low
##          0   10    4
##          1 1355 3370</code></pre>
<pre class="r"><code>summary(fit)</code></pre>
<pre><code>##
## Call:
## glm(formula = income ~ distance + score, family =
&quot;binomial&quot;,
## data = CollegeDistance)
##
## Deviance Residuals:
## Min 1Q Median 3Q Max
## -2.1863 -1.3276 0.7064 0.8621 1.2040
##
## Coefficients:
## Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept) 3.121833 0.207771 15.025 &lt; 2e-16 ***
## distance 0.079470 0.016614 4.783 1.72e-06 ***
## score -0.045553 0.003865 -11.786 &lt; 2e-16 ***
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## (Dispersion parameter for binomial family taken to be 1)
##
## Null deviance: 5690.4 on 4738 degrees of freedom
## Residual deviance: 5511.9 on 4736 degrees of freedom
## AIC: 5517.9
##
## Number of Fisher Scoring iterations: 4</code></pre>
<pre class="r"><code>CollegeDistance$logit &lt;-predict(fit)
ggplot(CollegeDistance,aes(logit, fill=income))+geom_density(alpha=.4)</code></pre>
<p><img src="/project/project2_files/figure-html/unnamed-chunk-8-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>library(plotROC)
ROCplot &lt;- ggplot(CollegeDistance) + geom_roc(aes(d=income, m=prob), n.cuts=0)
ROCplot</code></pre>
<p><img src="/project/project2_files/figure-html/unnamed-chunk-8-2.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>calc_auc(ROCplot)</code></pre>
<pre><code>##   PANEL group       AUC
## 1     1    -1 0.6220663</code></pre>
<p><strong>The log of odds of having low income goes up by 0.079 per 10 miles from a 4-year college and decreases by 0.045 per point on the achievement test. The accuracy of the data is 0.713, sensitivity is 0.999, specificity is 0.007, precision is 0.713, and auc is 0.622. The AUC indicates that the model is poor.</strong></p>
<ul>
<li><strong>6. Logistic Regression for All Variables</strong></li>
</ul>
<pre class="r"><code>fit &lt;- glm(income~., data=CollegeDistance, family = &quot;binomial&quot;)
prob &lt;- predict(fit, type = &quot;response&quot;)
class_diag(prob, CollegeDistance$income)</code></pre>
<pre><code>##           acc      sens      spec       ppv       auc
## low 0.7651403 0.9208654 0.3802198 0.7859853 0.7504172</code></pre>
<pre class="r"><code>truth &lt;- CollegeDistance$income
table(prediction = as.numeric(prob &gt; 0.5), truth)</code></pre>
<pre><code>##           truth
## prediction high  low
##          0  519  267
##          1  846 3107</code></pre>
<pre class="r"><code>set.seed(1234)
k=10
data&lt;-CollegeDistance[sample(nrow(CollegeDistance)),] #randomly order 
rowsfolds&lt;-cut(seq(1:nrow(CollegeDistance)),breaks=k,labels=F) #create folds

diags&lt;-NULL
for(i in 1:k){
   train&lt;-data[rowsfolds!=i,]   
   test&lt;-data[rowsfolds==i,]  
   truth&lt;-test$income
   fit1&lt;-glm(income~.,data=train,family=&quot;binomial&quot;)
   probs1&lt;-predict(fit1,newdata = test,type=&quot;response&quot;)
   diags&lt;-rbind(diags,class_diag(probs1,truth))
}
summarize_all(diags,mean)</code></pre>
<pre><code>##         acc      sens      spec       ppv       auc
## 1 0.7645048 0.9205399 0.3795037 0.7856159 0.7457492</code></pre>
<p><strong>The accuracy is 0.765, sensitivity is 0.921, specificity is 0.380, precision is 0.786, and auc is 0.746.The classification diagnostic variables were all relatively the same as the previous models. The AUC indicates that the model is a fair fit for the data.</strong></p>
<pre class="r"><code>library(glmnet)
set.seed(1234)
y&lt;-as.matrix(CollegeDistance$income) #grab response
x&lt;-model.matrix(income~.,data=CollegeDistance)[,-1] #grab predictors
cv&lt;-cv.glmnet(x,y,family=&quot;binomial&quot;)
lasso&lt;-glmnet(x,y,family=&quot;binomial&quot;,lambda=cv$lambda.1se)
coef(lasso)</code></pre>
<pre><code>## 19 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                              s0
## (Intercept)        3.754327e+00
## X                  .           
## gendermale        -6.545304e-02
## ethnicityhispanic  5.452584e-03
## ethnicityother    -4.288223e-01
## score              .           
## fcollegeyes       -1.201607e+00
## mcollegeyes       -4.600653e-01
## homeyes           -4.804713e-01
## urbanyes           5.791730e-02
## unemp              1.447289e-02
## wage              -3.162527e-02
## distance           1.437065e-02
## tuition            .           
## education         -1.163532e-01
## regionwest         .           
## distance_c         1.805785e-05
## score_c            .           
## logit              2.810141e-02</code></pre>
<pre class="r"><code>probs2&lt;-predict(lasso,x,type=&quot;response&quot;)
class_diag(probs2, CollegeDistance$income)</code></pre>
<pre><code>##           acc      sens    spec       ppv       auc
## low 0.7634522 0.9253112 0.36337 0.7822601 0.7451598</code></pre>
<pre class="r"><code>table(prediction = as.numeric(probs2 &gt; 0.5), CollegeDistance$income)</code></pre>
<pre><code>##           
## prediction high  low
##          0  496  252
##          1  869 3122</code></pre>
<p><strong>The retained variables after LASSO were gender, ethnicity, fcollege, mcollege, home, urban, unemployment, wage, distance, and education.</strong></p>
<pre class="r"><code>set.seed(1234)
k=10
data&lt;-CollegeDistance[sample(nrow(CollegeDistance)),] #randomly order 
rowsfolds&lt;-cut(seq(1:nrow(CollegeDistance)),breaks=k,labels=F) #create folds

diags&lt;-NULL
for(i in 1:k){
   train&lt;-data[rowsfolds!=i,]   
   test&lt;-data[rowsfolds==i,]  
   truth&lt;-test$income
   fit1&lt;-glm(income~gender+ethnicity+fcollege+mcollege+home+urban+unemp+wage+distance+education,data=train,family=&quot;binomial&quot;)
   probs1&lt;-predict(fit1,newdata = test,type=&quot;response&quot;)
   diags&lt;-rbind(diags,class_diag(probs1,truth))
}
summarize_all(diags,mean)</code></pre>
<pre><code>##         acc      sens      spec       ppv       auc
## 1 0.7636609 0.9208232 0.3757719 0.7846296 0.7461891</code></pre>
<p><strong>The accuracy is 0.764, sensitivity is 0.921, specificity is 0.376, precision is 0.785, and auc is 0.746.The classification diagnostic variables were all relatively the same as the previous models. The AUC indicates that the model is a fair fit for the data.</strong></p>
<p>…</p>
</div>
